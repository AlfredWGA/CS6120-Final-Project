{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"LSTM.ipynb","provenance":[{"file_id":"1HoK5ip4UraCuGdVP0sf8pAJSRFgAcaL8","timestamp":1619653915982},{"file_id":"1q1G28LKe1POcygN6C3iFe0FCPoEk3Pik","timestamp":1619650193426},{"file_id":"1MHsVpsRxZluLHqYfAaY4iKrSCRs939us","timestamp":1618966746898}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCkkzl5qOV4W","executionInfo":{"status":"ok","timestamp":1619721638686,"user_tz":240,"elapsed":21643,"user":{"displayName":"Logan Tang","photoUrl":"","userId":"16215472149116219288"}},"outputId":"70d9c3ec-8a0f-44cd-a460-0cfb1a92398d"},"source":["import os\n","from google.colab import drive\n","from argparse import Namespace\n","import numpy as np\n","import pandas as pd\n","import httpimport\n","import torch\n","import torch.optim as optim\n","from tqdm import tqdm_notebook, tqdm\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","drive.mount('/content/drive')\n","# path = 'drive/My Drive/blogtext.csv'\n","path = 'drive/My Drive/blogtext_preprocessed.csv'\n","# blog_data = pd.read_csv(path)\n","if torch.cuda.is_available():\n","  device = torch.device('cuda:0')\n","  print('gpu')\n","else:\n","  print('cpu')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","gpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pzf1q1Y37tP8"},"source":["Define Functions and Classes"]},{"cell_type":"code","metadata":{"id":"g_XSFERIRYsq"},"source":["class Preprocessing:\n","\t\n","\tdef __init__(self,args):\n","\t\tself.data = path\n","\t\tself.max_len = args.max_len\n","\t\tself.max_words = args.max_words\n","\t\tself.test_size = 0.25\n","\t\t\n","\tdef load_data(self):\n","\t\tdf = pd.read_csv(self.data).iloc[:10000,:]\n","\t\tdf = df.drop(['Unnamed: 0'], axis=1).dropna().reset_index(drop=True)\n","\t\tdf.replace({'gender': {'male': 1,'female': 0}},inplace=True)\n","\n","\t\tX = df['text'].values\n","\t\tY = df['gender'].values\n","\t\t\n","\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n","\t\t\n","\tdef prepare_tokens(self):\n","\t\tself.tokens = Tokenizer(num_words=self.max_words)\n","\t\tself.tokens.fit_on_texts(self.x_train)\n","\n","\tdef sequence_to_token(self, x):\n","\t\tsequences = self.tokens.texts_to_sequences(x)\n","\t\treturn sequence.pad_sequences(sequences, maxlen=self.max_len)\n","\n","class TextClassifier(nn.ModuleList):\n","\n","\tdef __init__(self, args):\n","\t\tsuper(TextClassifier, self).__init__()\n","\t\t\n","\t\t# Hyperparameters\n","\t\tself.batch_size = args.batch_size\n","\t\tself.hidden_dim = args.hidden_dim\n","\t\tself.LSTM_layers = args.lstm_layers\n","\t\tself.input_size = args.max_words\n","    # self.device = args.device\n","\t\t\n","\t\tself.dropout = nn.Dropout(0.5)\n","\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n","\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True) #hidden_size = round(x_train.shape[0]/(2*self.hidden_dim*self.hidden_dim))\n","\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim*2)\n","\t\tself.fc2 = nn.Linear(self.hidden_dim*2, 1)\n","\t\t\n","\tdef forward(self, x):\n","\t\t\n","\t\t# Hidden and cell state definion\n","\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).to(device) #manually set device\n","\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).to(device) #manually set device\n","\t\t\n","\t\t# Initialization fo hidden and cell states\n","\t\ttorch.nn.init.xavier_normal_(h).to(device) #manually set device\n","\t\ttorch.nn.init.xavier_normal_(c).to(device) #manually set device\n","\n","\t\t# Each sequence \"x\" is passed through an embedding layer\n","\t\tout = self.embedding(x)\n","\t\t# Feed LSTMs\n","\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n","\t\tout = self.dropout(out)\n","\t\t# The last hidden state is taken\n","\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n","\t\tout = self.dropout(out)\n","\t\tout = torch.sigmoid(self.fc2(out))\n","\n","\t\treturn out\n","\n","class DatasetMaper(Dataset):\n","\t'''\n","\tHandles batches of dataset\n","\t'''\n","  \n","\tdef __init__(self, x, y):\n","\t\tself.x = x\n","\t\tself.y = y\n","\t\t\n","\tdef __len__(self):\n","\t\treturn len(self.x)\n","\t\t\n","\tdef __getitem__(self, idx):\n","\t\treturn self.x[idx], self.y[idx]\n","\n","def evaluation():\n","  predictions = []\n","  \n","      # The model is turned in evaluation mode\n","  model.eval()\n","  \n","        # Skipping gradients update\n","  with torch.no_grad():\n","    \n","              # Iterate over the DataLoader object\n","    for x_batch, y_batch in loader_test:\n","      \n","      x = x_batch.type(torch.LongTensor).to(device) #manually set device\n","      y = y_batch.type(torch.FloatTensor).to(device) #manually set device\n","      \n","                  # Feed the model\n","      y_pred = model(x)\n","      \n","                  # Save prediction\n","      predictions += list(y_pred.detach().cpu().data.numpy()) #manually set device\n","      \n","  return predictions\n","\n","def calculate_accuray(grand_truth, predictions):\n","  true_positives = 0\n","  true_negatives = 0\n","  \n","  for true, pred in zip(grand_truth, predictions):\n","    if (pred > 0.5) and (true == 1):\n","      true_positives += 1\n","    elif (pred < 0.5) and (true == 0):\n","      true_negatives += 1\n","    else:\n","      pass\n","      \n","  return (true_positives+true_negatives) / len(grand_truth)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0ZHu1Er7v_4"},"source":["Modelling"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1ca24KtAY4c","executionInfo":{"status":"ok","timestamp":1619721695752,"user_tz":240,"elapsed":39537,"user":{"displayName":"Logan Tang","photoUrl":"","userId":"16215472149116219288"}},"outputId":"d48c1538-8270-425c-fe03-415943d8b641"},"source":["args = Namespace(\n","    epochs = 8,\n","    hidden_dim = 64,\n","    lstm_layers = 1,\n","    max_len = 300,\n","    max_words = 300,\n","    device = device,\n","    learning_rate = 0.001,\n","    batch_size = 64,\n","    early_stopping_criteria = 5\n",")\n","\n","preprocessing = Preprocessing(args)\n","preprocessing.load_data()\n","preprocessing.prepare_tokens()\n","x_train = preprocessing.sequence_to_token(preprocessing.x_train)\n","x_test = preprocessing.sequence_to_token(preprocessing.x_test)\n","y_train = preprocessing.y_train\n","y_test = preprocessing.y_test\n","\n","training_set = DatasetMaper(x_train, y_train)\n","test_set = DatasetMaper(x_test, y_test)\n","\n","model = TextClassifier(args).to(device) #manually set device\n","next(model.parameters()).device\n","loader_training = DataLoader(training_set, batch_size=model.batch_size)\n","loader_test = DataLoader(test_set)\n","\n","# Defines a RMSprop optimizer to update the parameters\n","optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n","\n","for epoch in tqdm(range(args.epochs)):\n","\n","  predictions = []\n","\n","  # model in training mode\n","  model.train()\n","\n","  for x_batch, y_batch in loader_training:\n","\n","    x = x_batch.type(torch.LongTensor).to(device) #manually set device\n","    # y = y_batch.type(torch.FloatTensor)\n","    y = torch.reshape(y_batch.type(torch.FloatTensor), (len(y_batch), 1)).to(device) #manually set device\n","\n","    # Feed the model and get output \"y_pred\"\n","    y_pred = model(x)\n","\n","    # Calculate loss\n","    loss = F.binary_cross_entropy(y_pred, y)\n","\n","    # The gradientes are calculated\n","    # i.e. derivates are calculated\n","    loss.backward()\n","    \n","    # Each parameter is updated\n","    # with torch.no_grad():\n","    #     a -= lr * a.grad\n","    #     b -= lr * b.grad\n","    optimizer.step()\n","    \n","    # Take the gradients to zero!\n","    # a.grad.zero_()\n","    # b.grad.zero_()\n","    optimizer.zero_grad()\n","\n","raw_predictions = evaluation()\n","predictions = [raw_predictions[i][0] for i in range(len(raw_predictions))]\n","\n","print('Accuracy is {}'.format(calculate_accuray(y_test, predictions)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 8/8 [00:13<00:00,  1.75s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy is 0.6946812829882257\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cafxfo8-7yv6"},"source":["Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYnauIe59dvy","executionInfo":{"status":"ok","timestamp":1619670644513,"user_tz":240,"elapsed":143497,"user":{"displayName":"Logan Tang","photoUrl":"","userId":"16215472149116219288"}},"outputId":"5e2d3daf-bc07-4cbd-ee66-02267a882013"},"source":["import itertools\n","best_acc = 0\n","best_param_combination = []\n","\n","parameter_grid = [[epochs,hidden_dim,lstm_layers,max_len,max_words] for epochs,hidden_dim,lstm_layers,max_len,max_words in itertools.product([8],[64,128,256],[1],[100,300],[300])]\n","for param_combination in tqdm(parameter_grid):\n","  args = Namespace(\n","      epochs = param_combination[0],\n","      hidden_dim = param_combination[1],\n","      lstm_layers = param_combination[2],\n","      max_len = param_combination[3],\n","      max_words = param_combination[4],\n","      device = device,\n","      learning_rate = 0.001,\n","      batch_size = 64,\n","      early_stopping_criteria = 5\n","  )\n","\n","  preprocessing = Preprocessing(args)\n","  preprocessing.load_data()\n","  preprocessing.prepare_tokens()\n","  x_train = preprocessing.sequence_to_token(preprocessing.x_train)\n","  x_test = preprocessing.sequence_to_token(preprocessing.x_test)\n","  y_train = preprocessing.y_train\n","  y_test = preprocessing.y_test\n","\n","  training_set = DatasetMaper(x_train, y_train)\n","  test_set = DatasetMaper(x_test, y_test)\n","\n","  model = TextClassifier(args).to(device) #manually set device\n","  next(model.parameters()).device\n","  loader_training = DataLoader(training_set, batch_size=model.batch_size)\n","  loader_test = DataLoader(test_set)\n","\n","  # Defines a RMSprop optimizer to update the parameters\n","  optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n","\n","  for epoch in tqdm(range(args.epochs)):\n","\n","    predictions = []\n","\n","    # model in training mode\n","    model.train()\n","\n","    for x_batch, y_batch in loader_training:\n","\n","      x = x_batch.type(torch.LongTensor).to(device) #manually set device\n","\n","      y = torch.reshape(y_batch.type(torch.FloatTensor), (len(y_batch), 1)).to(device) #manually set device\n","\n","      y_pred = model(x)\n","\n","      loss = F.binary_cross_entropy(y_pred, y)\n","\n","      loss.backward()\n","      \n","      optimizer.step()\n","\n","      optimizer.zero_grad()\n","\n","  raw_predictions = evaluation()\n","  predictions = [raw_predictions[i][0] for i in range(len(raw_predictions))]\n","  if calculate_accuray(y_test, predictions)>best_acc:\n","    best_acc = calculate_accuray(y_test, predictions)\n","    best_param_combination = param_combination\n","print('best acc is {} with hyperparameters {}'.format(best_acc,best_param_combination))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n"," 12%|█▎        | 1/8 [00:00<00:04,  1.47it/s]\u001b[A\u001b[A\n","\n"," 25%|██▌       | 2/8 [00:01<00:04,  1.48it/s]\u001b[A\u001b[A\n","\n"," 38%|███▊      | 3/8 [00:02<00:03,  1.49it/s]\u001b[A\u001b[A\n","\n"," 50%|█████     | 4/8 [00:02<00:02,  1.50it/s]\u001b[A\u001b[A\n","\n"," 62%|██████▎   | 5/8 [00:03<00:01,  1.50it/s]\u001b[A\u001b[A\n","\n"," 75%|███████▌  | 6/8 [00:03<00:01,  1.50it/s]\u001b[A\u001b[A\n","\n"," 88%|████████▊ | 7/8 [00:04<00:00,  1.50it/s]\u001b[A\u001b[A\n","\n","100%|██████████| 8/8 [00:05<00:00,  1.50it/s]\n","\n"," 17%|█▋        | 1/6 [00:13<01:06, 13.23s/it]\u001b[A\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n"," 12%|█▎        | 1/8 [00:01<00:10,  1.55s/it]\u001b[A\u001b[A\n","\n"," 25%|██▌       | 2/8 [00:03<00:09,  1.55s/it]\u001b[A\u001b[A\n","\n"," 38%|███▊      | 3/8 [00:04<00:07,  1.55s/it]\u001b[A\u001b[A\n","\n"," 50%|█████     | 4/8 [00:06<00:06,  1.55s/it]\u001b[A\u001b[A\n","\n"," 62%|██████▎   | 5/8 [00:07<00:04,  1.54s/it]\u001b[A\u001b[A\n","\n"," 75%|███████▌  | 6/8 [00:09<00:03,  1.54s/it]\u001b[A\u001b[A\n","\n"," 88%|████████▊ | 7/8 [00:10<00:01,  1.54s/it]\u001b[A\u001b[A\n","\n","100%|██████████| 8/8 [00:12<00:00,  1.54s/it]\n","\n"," 33%|███▎      | 2/6 [00:37<01:06, 16.58s/it]\u001b[A\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n"," 12%|█▎        | 1/8 [00:00<00:05,  1.23it/s]\u001b[A\u001b[A\n","\n"," 25%|██▌       | 2/8 [00:01<00:04,  1.23it/s]\u001b[A\u001b[A\n","\n"," 38%|███▊      | 3/8 [00:02<00:04,  1.23it/s]\u001b[A\u001b[A\n","\n"," 50%|█████     | 4/8 [00:03<00:03,  1.23it/s]\u001b[A\u001b[A\n","\n"," 62%|██████▎   | 5/8 [00:04<00:02,  1.23it/s]\u001b[A\u001b[A\n","\n"," 75%|███████▌  | 6/8 [00:04<00:01,  1.23it/s]\u001b[A\u001b[A\n","\n"," 88%|████████▊ | 7/8 [00:05<00:00,  1.23it/s]\u001b[A\u001b[A\n","\n","100%|██████████| 8/8 [00:06<00:00,  1.23it/s]\n","\n"," 50%|█████     | 3/6 [00:52<00:47, 15.94s/it]\u001b[A\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n"," 12%|█▎        | 1/8 [00:02<00:14,  2.12s/it]\u001b[A\u001b[A\n","\n"," 25%|██▌       | 2/8 [00:04<00:12,  2.13s/it]\u001b[A\u001b[A\n","\n"," 38%|███▊      | 3/8 [00:06<00:10,  2.13s/it]\u001b[A\u001b[A\n","\n"," 50%|█████     | 4/8 [00:08<00:08,  2.13s/it]\u001b[A\u001b[A\n","\n"," 62%|██████▎   | 5/8 [00:10<00:06,  2.14s/it]\u001b[A\u001b[A\n","\n"," 75%|███████▌  | 6/8 [00:12<00:04,  2.14s/it]\u001b[A\u001b[A\n","\n"," 88%|████████▊ | 7/8 [00:14<00:02,  2.14s/it]\u001b[A\u001b[A\n","\n","100%|██████████| 8/8 [00:17<00:00,  2.14s/it]\n","\n"," 67%|██████▋   | 4/6 [01:21<00:39, 19.87s/it]\u001b[A\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n"," 12%|█▎        | 1/8 [00:01<00:09,  1.36s/it]\u001b[A\u001b[A\n","\n"," 25%|██▌       | 2/8 [00:02<00:08,  1.35s/it]\u001b[A\u001b[A\n","\n"," 38%|███▊      | 3/8 [00:04<00:06,  1.35s/it]\u001b[A\u001b[A\n","\n"," 50%|█████     | 4/8 [00:05<00:05,  1.35s/it]\u001b[A\u001b[A\n","\n"," 62%|██████▎   | 5/8 [00:06<00:04,  1.35s/it]\u001b[A\u001b[A\n","\n"," 75%|███████▌  | 6/8 [00:08<00:02,  1.35s/it]\u001b[A\u001b[A\n","\n"," 88%|████████▊ | 7/8 [00:09<00:01,  1.35s/it]\u001b[A\u001b[A\n","\n","100%|██████████| 8/8 [00:10<00:00,  1.35s/it]\n","\n"," 83%|████████▎ | 5/6 [01:39<00:19, 19.55s/it]\u001b[A\n","\n","  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n"," 12%|█▎        | 1/8 [00:03<00:25,  3.64s/it]\u001b[A\u001b[A\n","\n"," 25%|██▌       | 2/8 [00:07<00:21,  3.64s/it]\u001b[A\u001b[A\n","\n"," 38%|███▊      | 3/8 [00:10<00:18,  3.64s/it]\u001b[A\u001b[A\n","\n"," 50%|█████     | 4/8 [00:14<00:14,  3.64s/it]\u001b[A\u001b[A\n","\n"," 62%|██████▎   | 5/8 [00:18<00:10,  3.64s/it]\u001b[A\u001b[A\n","\n"," 75%|███████▌  | 6/8 [00:21<00:07,  3.65s/it]\u001b[A\u001b[A\n","\n"," 88%|████████▊ | 7/8 [00:25<00:03,  3.65s/it]\u001b[A\u001b[A\n","\n","100%|██████████| 8/8 [00:29<00:00,  3.65s/it]\n","\n","100%|██████████| 6/6 [02:23<00:00, 23.86s/it]"],"name":"stderr"},{"output_type":"stream","text":["best acc is 0.6999593991067804 with hyperparameters [8, 64, 1, 300, 300]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}