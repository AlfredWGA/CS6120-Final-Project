{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMj1u3lYbtyKRAtRR5R4MOd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NUWrgeXNj2e","executionInfo":{"status":"ok","timestamp":1619709632519,"user_tz":240,"elapsed":4736,"user":{"displayName":"Logan Tang","photoUrl":"","userId":"16215472149116219288"}},"outputId":"ca7473f4-d808-42fd-b77b-ae5d1dd472dd"},"source":["import re\n","import nltk\n","from argparse import Namespace\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from nltk.tokenize import word_tokenize\n","from keras.preprocessing.text import Tokenizer\n","import os\n","from google.colab import drive\n","from argparse import Namespace\n","import torch\n","import torch.optim as optim\n","from tqdm import tqdm_notebook, tqdm\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","import torch.nn.functional as F\n","import math\n","\n","drive.mount('/content/drive')\n","path = 'drive/My Drive/blogtext_preprocessed.csv'\n","# blog_data = pd.read_csv(path).iloc[:10000,]\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"3I-fhiJm6R6x"},"source":["Define Functions and Classes"]},{"cell_type":"code","metadata":{"id":"zw3GpGG5O-aJ"},"source":["class Preprocessing:\n","\t\n","\tdef __init__(self,args):\n","\t\tself.data = path\n","\t\tself.max_len = args.max_len\n","\t\tself.max_words = args.max_words\n","\t\tself.test_size = 0.25\n","\t\t\n","\tdef load_data(self):\n","\t\tdf = pd.read_csv(self.data).iloc[:10000,:]\n","\t\tdf = df.drop(['Unnamed: 0'], axis=1).dropna().reset_index(drop=True)\n","\t\tdf.replace({'gender': {'male': 1,'female': 0}},inplace=True)\n","\n","\t\tX = df['text'].values\n","\t\tY = df['gender'].values\n","\t\t\n","\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n","\t\t\n","\tdef prepare_tokens(self):\n","\t\tself.tokens = Tokenizer(num_words=self.max_words)\n","\t\tself.tokens.fit_on_texts(self.x_train)\n","\n","\tdef sequence_to_token(self, x):\n","\t\tsequences = self.tokens.texts_to_sequences(x)\n","\t\treturn sequence.pad_sequences(sequences, maxlen=self.max_len)\n","  \n","class DatasetMaper(Dataset):\n","\t'''\n","\tHandles batches of dataset\n","\t'''\n","  \n","\tdef __init__(self, x, y):\n","\t\tself.x = x\n","\t\tself.y = y\n","\t\t\n","\tdef __len__(self):\n","\t\treturn len(self.x)\n","\t\t\n","\tdef __getitem__(self, idx):\n","\t\treturn self.x[idx], self.y[idx]\n","\n","class TextClassifier(nn.ModuleList):\n","  def __init__(self, params):\n","    super(TextClassifier, self).__init__()\n","\n","    # Parameters regarding text preprocessing\n","    self.seq_len = params.max_len\n","    self.num_words = params.max_words\n","    self.embedding_size = params.embedding_size\n","    \n","    # Dropout definition\n","    self.dropout = nn.Dropout(0.25)\n","    \n","    # CNN parameters definition\n","    # Kernel sizes\n","    self.kernel_1 = 2\n","    self.kernel_2 = 3\n","    self.kernel_3 = 4\n","    self.kernel_4 = 5\n","    \n","    # Output size for each convolution\n","    self.out_size = params.out_size\n","    # Number of strides for each convolution\n","    self.stride = params.stride\n","    \n","    # Embedding layer definition\n","    self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n","    \n","    # Convolution layers definition\n","    self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n","    self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n","    self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n","    self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n","    \n","    # Max pooling layers definition\n","    self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n","    self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n","    self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n","    self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n","    \n","    # Fully connected layer definition\n","    self.fc = nn.Linear(self.in_features_fc(), 1)\n","\n","  def in_features_fc(self):\n","    # Calcualte size of convolved/pooled features for convolution_1/max_pooling_1 features\n","    out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n","    out_conv_1 = math.floor(out_conv_1)\n","    out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n","    out_pool_1 = math.floor(out_pool_1)\n","    \n","      # Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n","    out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n","    out_conv_2 = math.floor(out_conv_2)\n","    out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n","    out_pool_2 = math.floor(out_pool_2)\n","    \n","      # Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n","    out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n","    out_conv_3 = math.floor(out_conv_3)\n","    out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n","    out_pool_3 = math.floor(out_pool_3)\n","    \n","      # Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n","    out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n","    out_conv_4 = math.floor(out_conv_4)\n","    out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n","    out_pool_4 = math.floor(out_pool_4)\n","    \n","      # Returns \"flattened\" vector (input for fully connected layer)\n","    return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n","\n","  def forward(self, x):\n","\n","      # Sequence of tokes is filterd through an embedding layer\n","    x = self.embedding(x)\n","      \n","      # Convolution layer 1 is applied\n","    x1 = self.conv_1(x)\n","    x1 = torch.relu(x1)\n","    x1 = self.pool_1(x1)\n","      \n","      # Convolution layer 2 is applied\n","    x2 = self.conv_2(x)\n","    x2 = torch.relu((x2))\n","    x2 = self.pool_2(x2)\n","   \n","      # Convolution layer 3 is applied\n","    x3 = self.conv_3(x)\n","    x3 = torch.relu(x3)\n","    x3 = self.pool_3(x3)\n","      \n","      # Convolution layer 4 is applied\n","    x4 = self.conv_4(x)\n","    x4 = torch.relu(x4)\n","    x4 = self.pool_4(x4)\n","      \n","      # The output of each convolutional layer is concatenated into a unique vector\n","    union = torch.cat((x1, x2, x3, x4), 2)\n","    union = union.reshape(union.size(0), -1)\n","\n","      # The \"flattened\" vector is passed through a fully connected layer\n","    out = self.fc(union)\n","      # Dropout is applied\t\t\n","    out = self.dropout(out)\n","      # Activation function is applied\n","    out = torch.sigmoid(out)\n","      \n","    return out.squeeze()\n","\n","def train(model, x_train, y_train, x_test, y_test, params):\n","   \n","  #  # Initialize dataset maper\n","   train = DatasetMaper(x_train, y_train)\n","   test = DatasetMaper(x_test, y_test)\n","   \n","  #  # Initialize loaders\n","   loader_train = DataLoader(train, batch_size=params.batch_size)\n","   loader_test = DataLoader(test, batch_size=params.batch_size)\n","   \n","   # Define optimizer\n","   optimizer = optim.RMSprop(model.parameters(), lr=params.learning_rate)\n","   \n","   # Starts training phase\n","   for epoch in range(params.epochs):\n","      # Set model in training model\n","      model.train()\n","      predictions = []\n","      # Starts batch training\n","      for x_batch, y_batch in loader_train:\n","      \n","         y_batch = y_batch.type(torch.FloatTensor)\n","         \n","         # Feed the model\n","         y_pred = model(x_batch)\n","         \n","         # Loss calculation\n","         loss = F.binary_cross_entropy(y_pred, y_batch)\n","         \n","         # Clean gradientes\n","         optimizer.zero_grad()\n","         \n","         # Gradients calculation\n","         loss.backward()\n","         \n","         # Gradients update\n","         optimizer.step()\n","         \n","         # Save predictions\n","         predictions += list(y_pred.detach().numpy())\n","      \n","      # Evaluation phase\n","      test_predictions = Run.evaluation(model, loader_test)\n","      \n","      # Metrics calculation\n","      train_accuary = Run.calculate_accuray(data['y_train'], predictions)\n","      test_accuracy = Run.calculate_accuray(data['y_test'], test_predictions)\n","      print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n","\n","def calculate_accuray(grand_truth, predictions):\n","   true_positives = 0\n","   true_negatives = 0\n","   \n","   # Gets frequency  of true positives and true negatives\n","   # The threshold is 0.5\n","   for true, pred in zip(grand_truth, predictions):\n","      if (pred >= 0.5) and (true == 1):\n","         true_positives += 1\n","      elif (pred < 0.5) and (true == 0):\n","         true_negatives += 1\n","      else:\n","         pass\n","   # Return accuracy\n","   return (true_positives+true_negatives) / len(grand_truth)\n","\n","def evaluation(model, loader_test):\n","\t\t\n","\t\t# Set the model in evaluation mode\n","\t\tmodel.eval()\n","\t\tpredictions = []\n","\t\t\n","\t\t# Starst evaluation phase\n","\t\twith torch.no_grad():\n","\t\t\tfor x_batch, y_batch in loader_test:\n","\t\t\t\tx = x_batch.type(torch.LongTensor)\n","\t\t\t\ty = y_batch.type(torch.FloatTensor)\n","\t\t\t\ty_pred = model(x_batch)\n","\t\t\t\t# predictions += list(y_pred.detach().data.numpy())\n","\t\t\t\tpredictions.append(y_pred.detach().data.numpy())\n","\t\treturn predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HUiwnvbt6WQ1"},"source":["Modelling"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zxkw6m1N5aCh","executionInfo":{"status":"ok","timestamp":1619727530761,"user_tz":240,"elapsed":284312,"user":{"displayName":"Logan Tang","photoUrl":"","userId":"16215472149116219288"}},"outputId":"60469ed0-3d36-4fda-9b89-9af2f39dc889"},"source":["args = Namespace(\n","    max_len = 1000,\n","    max_words = 300,\n","    learning_rate = 0.001,\n","    batch_size = 64,\n","    embedding_size = 128,\n","    out_size = 32,\n","    stride = 2,\n","    epochs = 4\n",")\n","\n","preprocessing = Preprocessing(args)\n","preprocessing.load_data()\n","preprocessing.prepare_tokens()\n","x_train = preprocessing.sequence_to_token(preprocessing.x_train)\n","x_test = preprocessing.sequence_to_token(preprocessing.x_test)\n","y_train = preprocessing.y_train\n","y_test = preprocessing.y_test\n","\n","training_set = DatasetMaper(x_train, y_train)\n","test_set = DatasetMaper(x_test, y_test)\n","loader_train = DataLoader(training_set, batch_size=args.batch_size)\n","loader_test = DataLoader(test_set)\n","\n","model = TextClassifier(args)\n","\n","# Define optimizer\n","optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n","# Starts training phase\n","for epoch in tqdm(range(args.epochs)):\n","  # Set model in training model\n","  model.train()\n","  predictions = []\n","  # Starts batch training\n","  for x_batch, y_batch in loader_train:\n","      x_batch = x_batch.type(torch.LongTensor)\n","      y_batch = y_batch.type(torch.FloatTensor)\n","      \n","      # Feed the model\n","      y_pred = model(x_batch)\n","      \n","      # Loss calculation\n","      loss = F.binary_cross_entropy(y_pred, y_batch)\n","      \n","      # Clean gradientes\n","      optimizer.zero_grad()\n","      \n","      # Gradients calculation\n","      loss.backward()\n","      \n","      # Gradients update\n","      optimizer.step()\n","      \n","      # Save predictions\n","      predictions += list(y_pred.detach().numpy())\n","  \n","  # Evaluation phase\n","  test_predictions = evaluation(model, loader_test)\n","  \n","  # Metrics calculation\n","  train_accuary = calculate_accuray(y_train, predictions)\n","  test_accuracy = calculate_accuray(y_test, test_predictions)\n","  print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 25%|██▌       | 1/4 [01:09<03:27, 69.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1, loss: 0.66120, Train accuracy: 0.59196, Test accuracy: 0.60983\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|█████     | 2/4 [02:18<02:18, 69.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 2, loss: 0.52192, Train accuracy: 0.67790, Test accuracy: 0.66910\n"],"name":"stdout"},{"output_type":"stream","text":["\r 75%|███████▌  | 3/4 [03:28<01:09, 69.51s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 3, loss: 0.65159, Train accuracy: 0.72243, Test accuracy: 0.64515\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 4/4 [04:38<00:00, 69.67s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 4, loss: 0.47873, Train accuracy: 0.75572, Test accuracy: 0.67073\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"zPK4Mu6o6Fnx"},"source":["Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qeaAwG59Zu0","executionInfo":{"status":"ok","timestamp":1619727060676,"user_tz":240,"elapsed":2052121,"user":{"displayName":"Logan Tang","photoUrl":"","userId":"16215472149116219288"}},"outputId":"5d3de99b-6a80-43a7-9f94-a51ce370cdfd"},"source":["import itertools\n","best_acc = 0\n","best_param_combination = []\n","\n","parameter_grid = [[max_len,max_words,embedding_size] for max_len,max_words,embedding_size in itertools.product([300,500,1000],[100,200,300],[32,64,128])]\n","\n","for param_combination in tqdm(parameter_grid):\n","  args = Namespace(\n","    max_len = param_combination[0],\n","    max_words = param_combination[1],\n","    learning_rate = 0.001,\n","    batch_size = 64,\n","    embedding_size = param_combination[2],\n","    out_size = 32,\n","    stride = 2,\n","    epochs = 4\n","  )\n","\n","  preprocessing = Preprocessing(args)\n","  preprocessing.load_data()\n","  preprocessing.prepare_tokens()\n","  x_train = preprocessing.sequence_to_token(preprocessing.x_train)\n","  x_test = preprocessing.sequence_to_token(preprocessing.x_test)\n","  y_train = preprocessing.y_train\n","  y_test = preprocessing.y_test\n","\n","  training_set = DatasetMaper(x_train, y_train)\n","  test_set = DatasetMaper(x_test, y_test)\n","  loader_train = DataLoader(training_set, batch_size=args.batch_size)\n","  loader_test = DataLoader(test_set)\n","\n","  model = TextClassifier(args)\n","\n","  # Define optimizer\n","  optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n","  # Starts training phase\n","  for epoch in range(args.epochs):\n","    # Set model in training model\n","    model.train()\n","    predictions = []\n","    # Starts batch training\n","    for x_batch, y_batch in loader_train:\n","        x_batch = x_batch.type(torch.LongTensor)\n","        y_batch = y_batch.type(torch.FloatTensor)\n","        \n","        # Feed the model\n","        y_pred = model(x_batch)\n","        \n","        # Loss calculation\n","        loss = F.binary_cross_entropy(y_pred, y_batch)\n","        \n","        # Clean gradientes\n","        optimizer.zero_grad()\n","        \n","        # Gradients calculation\n","        loss.backward()\n","        \n","        # Gradients update\n","        optimizer.step()\n","        \n","        # Save predictions\n","        predictions += list(y_pred.detach().numpy())\n","    \n","    # Evaluation phase\n","    test_predictions = evaluation(model, loader_test)\n","    \n","    # Metrics calculation\n","    train_accuary = calculate_accuray(y_train, predictions)\n","    test_accuracy = calculate_accuray(y_test, test_predictions)\n","    # print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n","\n","  if test_accuracy>best_acc:\n","    best_acc = test_accuracy\n","    best_param_combination = param_combination\n","print('best acc is {} with hyperparameters {}'.format(best_acc,best_param_combination))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 27/27 [51:05<00:00, 113.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["best acc is 0.6605765326837191 with hyperparameters [1000, 300, 128]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}